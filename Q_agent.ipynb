{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from NN_models.Q_model import DQN as Q_model\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch\n",
    "import thread\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "use_GPU = torch.cuda.is_available()\n",
    "PILMODE='L'\n",
    "Image_Size=[240,240]\n",
    "input_shape = [24,240,240]\n",
    "actionsize=6\n",
    "loss_func=nn.MSELoss()\n",
    "if use_GPU:\n",
    "    loss_func = loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    model_path='DQN_models'\n",
    "    memory_path='RL_DATA/'\n",
    "    state_file=memory_path+'STATE'\n",
    "    action_file=memory_path+'ACTION'\n",
    "    ep_reward_file=memory_path+'ep_reward.dat'\n",
    "    batch_size = 25\n",
    "    epsilon = 1\n",
    "    epsilon_decay = 0.99\n",
    "    epsilon_final = 0.1\n",
    "    epsilon_endtime = 30000\n",
    "    action_size = 6\n",
    "    discount_factor = 0.7\n",
    "    n_replay = 1 #replay per learning step\n",
    "    clip_delta = 1\n",
    "\n",
    "    def __init__(self,episode=1):\n",
    "        self.ep = episode\n",
    "        if episode==1:\n",
    "            self.F_model = Q_model(output_shape=actionsize)\n",
    "            self.target_F_model = Q_model(output_shape=actionsize)\n",
    "        else:\n",
    "            self.load_model(episode)\n",
    "        if use_GPU:\n",
    "            self.F_model.cuda()\n",
    "            self.target_F_model.cuda()\n",
    "        self.F_optimizer = Adam(self.F_model.parameters(),1e-4)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        nsize = [1,]\n",
    "        nsize.extend(state.shape)\n",
    "        state = state.reshape(nsize)\n",
    "        state = Variable(torch.Tensor(state))\n",
    "        state = torch.Tensor(state)\n",
    "        if use_GPU:\n",
    "            state = state.cuda()\n",
    "        res = self.Y_model(state)\n",
    "        _,act = res.max()\n",
    "        return max\n",
    "    def get_action_eps(self,state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            #if self.epsilon > self.epsilon_final:\n",
    "                #self.epsilon*=self.epsilon_decay\n",
    "            return random.randint(0,self.action_size-1)\n",
    "        return self.get_action(state)\n",
    "            \n",
    "    \n",
    "    def memorize(self,state,action,n_state,ep,ts):\n",
    "        thread.start_new_thread(save_state,(state,ep,ts))\n",
    "        thread.start_new_thread(save_action,(action,ep,ts))\n",
    "\n",
    "    def save_model(self,tag=None):\n",
    "        name='episode_%d_model'%self.ep\n",
    "        if tag is not None:\n",
    "            name += tag\n",
    "        torch.save(self.target_F_model,name+'.model')\n",
    "        \n",
    "    def load_model(self):\n",
    "        name='episode_%d_model'%(self.ep-1)\n",
    "        self.F_model = torch.load(name)\n",
    "        self.target_F_model=torch.load(name)\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_F_model.load_state_dict(self.F_model.state_dict())\n",
    "        \n",
    "    def experience_replay(self,N=10,batchsize=25):\n",
    "        memory = load_memory_of_episode(self.ep)\n",
    "        for i in range(int(N*len(memory)/batchsize)):\n",
    "            minibatch = random.sample(memory,batchsize)\n",
    "            shape=[batchsize,]\n",
    "            shape.extend(input_shape)\n",
    "            update_input = np.zeros(shape)\n",
    "            update_target = np.zeros((batchsize, self.action_size))\n",
    "            for j in range(batchsize):\n",
    "                state = minibatch[j]['state']\n",
    "                action = int(minibatch[j]['action'])\n",
    "                reward = minibatch[j]['reward']\n",
    "                n_state = minibatch[j]['n_state']\n",
    "                \n",
    "                ystate = Variable(torch.FloatTensor(state))\n",
    "                nstate = Variable(torch.FloatTensor(n_state))\n",
    "                nshape=[1,]\n",
    "                nshape.extend(ystate.size())\n",
    "                ystate = ystate.view(nshape)\n",
    "                nstate = nstate.view(nshape)\n",
    "                if use_GPU:\n",
    "                    ystate = ystate.cuda()\n",
    "                    nstate = nstate.cuda()\n",
    "                target = self.F_model.forward(ystate)[0].cpu().data.numpy()\n",
    "                q_2 =self.target_F_model.forward(nstate)\n",
    "                q_2_max = torch.max(q_2).cpu().data.numpy()\n",
    "                target[action] = reward + self.discount_factor*q_2_max\n",
    "\n",
    "                update_input[i] = state\n",
    "                update_target[i] = target\n",
    "            \n",
    "            update_input=Variable(torch.FloatTensor(update_input))\n",
    "            update_target=Variable(torch.FloatTensor(update_target))\n",
    "            if use_GPU:\n",
    "                update_input=update_input.cuda()\n",
    "                update_target=update_target.cuda()\n",
    "            prediction=self.F_model.forward(update_input)\n",
    "            loss = loss_func(prediction,update_target)\n",
    "\n",
    "            self.F_model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            for param in self.F_model.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "            self.F_optimizer.step()\n",
    "        self.update_target_model()\n",
    "    \n",
    "def save_state(state,ep,time_stamp):\n",
    "    s_dir='RL_DATA/EP%d/STATE/s%04d'%(ep,time_stamp)\n",
    "    if not os.path.isdir(s_dir):\n",
    "        os.makedirs(s_dir)\n",
    "    for i in range(state.shape[0]):\n",
    "        frame=ToPILImage(state[i])\n",
    "        frame.save(s_dir+'/%04d.png'%i)\n",
    "def save_action(action,ep,time_stamp):\n",
    "    s_dir='RL_DATA/EP%d/ACTION'%ep\n",
    "    if not os.path.isdir(s_dir):\n",
    "        os.makedirs(s_dir)\n",
    "    actfil=open(s_dir+'/a%04d.data'%time_stamp,'w')\n",
    "    actfil.write(str(action))\n",
    "    actfil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.zeros((24,480,640))\n",
    "nsize = [1,]\n",
    "nsize.extend(state.shape)\n",
    "state = state.reshape(nsize)\n",
    "state = Variable(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import glob\n",
    "from model import *\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "use_GPU=torch.cuda.is_available()\n",
    "D=torch.load('iter_4650.model')\n",
    "C=torch.load('CNN_8613.model')#load classifier\n",
    "if use_GPU:\n",
    "    C.cuda()\n",
    "    D.cuda()\n",
    "D.eval()\n",
    "C.eval()\n",
    "N=100\n",
    "episode=int(open('RL_DATA/EP.data').readline())\n",
    "w_c=0.5\n",
    "w_d=1\n",
    "d_thres=0.2\n",
    "Image_Size=240\n",
    "UImage_Size=784\n",
    "input_shape = [24,240,240]\n",
    "def reform_one(data):\n",
    "    xl=data[:,::3]\n",
    "    yl=data[:,1::3]\n",
    "    add_dim = lambda x:x.reshape(x.shape[0],1,-1,1)\n",
    "    xl=add_dim(xl)\n",
    "    yl=add_dim(yl)\n",
    "    l = np.concatenate([xl,yl],3)\n",
    "    score=data[:,2::3]\n",
    "    score=score.reshape(score.shape[0],2,-1)\n",
    "    return l,score\n",
    "def normalization(cood,size):\n",
    "    cood=cood/size-0.5\n",
    "    return cood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory_of_episode(episode):\n",
    "    directory = 'RL_DATA/EP%d'%episode\n",
    "    #load states\n",
    "    s_dir = directory +'/STATE/'\n",
    "    s_list = glob.glob(s_dir+'s*')\n",
    "    s_list.sort()\n",
    "    states=list()\n",
    "    for folder in s_list:\n",
    "        frames = glob.glob(folder+'/*.png')\n",
    "        frames.sort()\n",
    "        state_list=list()\n",
    "        for frame in frames:\n",
    "            img = Image.open(open(frame))\n",
    "            img_arr = np.array(img)\n",
    "            state_list.append(img_arr)\n",
    "        states.append(np.array(state_list))\n",
    "    states = np.array(states)\n",
    "    #load actions\n",
    "    a_dir = directory +'/ACTION/'\n",
    "    a_list = glob.glob(a_dir+'*')\n",
    "    a_list.sort()\n",
    "    actions = list()\n",
    "    for fil in a_list:\n",
    "        afile = open(fil)\n",
    "        action = int(afile.readline())\n",
    "        afile.close()\n",
    "        actions.append(action)\n",
    "    #calculate reward\n",
    "    pose_list = s_list\n",
    "    pose_list.sort()\n",
    "    poses_o=list()\n",
    "    person_max=list()\n",
    "    for folder in pose_list:\n",
    "        posefils = glob.glob(folder+'/*.json')\n",
    "        posefils.sort()\n",
    "        pose_list=list()\n",
    "        p_max=0#count person\n",
    "        for posefil in posefils:\n",
    "            frame=json.load(open(posefil))\n",
    "            if len(frame['people']) >0:\n",
    "                p_max = len(frame['people'])\n",
    "                pose=frame['people'][0]['pose_keypoints_2d']\n",
    "            else:\n",
    "                pose = [Image_Size/2 for i in range(54)]\n",
    "            pose_list.append(pose)\n",
    "        pose_list = np.array(pose_list)\n",
    "        original,score=reform_one(pose_list)\n",
    "        processed=normalization(original,Image_Size)\n",
    "        poses_o.append(processed)\n",
    "    poses_o=np.array(poses_o)\n",
    "    #get action skeleton\n",
    "    action_skeleton_folder = 'robot/action_skeletons/'\n",
    "    poses_s=list()\n",
    "    for act in actions:\n",
    "        fildir = action_skeleton_folder+str(act)\n",
    "        posefils = glob.glob(fildir+'/*.json')\n",
    "        posefils.sort()\n",
    "        pose_list=list()\n",
    "        for posefil in posefils[:24]: # only need 24 frames\n",
    "            frame=json.load(open(posefil))\n",
    "            if len(frame['people']) >0:\n",
    "                pose=frame['people'][0]['pose_keypoints_2d']\n",
    "            else:\n",
    "                pose = [UImage_Size/2 for i in range(54)]\n",
    "            pose_list.append(pose)\n",
    "        pose_list = np.array(pose_list)\n",
    "        original,score=reform_one(pose_list)\n",
    "        processed=normalization(original,UImage_Size)\n",
    "        poses_s.append(processed)\n",
    "    poses_s=np.array(poses_s)\n",
    "    pair = np.concatenate((poses_o[1:],poses_s[:-1]),axis=2)\n",
    "    pair = Variable(torch.Tensor(pair))\n",
    "    if use_GPU:\n",
    "        pair = pair.cuda()\n",
    "#######################################################################todo\n",
    "    C_score = F.softmax(C(pair)).cpu().data.numpy()\n",
    "    m=np.max(C_score,axis=1)\n",
    "    agm=np.argmax(C_score,axis=1)\n",
    "    for i in range(len(agm)):\n",
    "        if agm[i]==7:# recognized as unknown interaction\n",
    "            m[i]=0\n",
    "    C_reward=m\n",
    "    D_reward = F.sigmoid((D(pair)[0] -0.2)).cpu().data.numpy().reshape(-1)\n",
    "    reward =  w_c*C_reward+w_d*D_reward\n",
    "    print(reward)\n",
    "#######################################################################\n",
    "    memories = list()\n",
    "    for i in range(len(actions)-1):\n",
    "        memories.append(dict(state =states[i],action=actions[i],reward=reward[i],n_state=states[i+1]))\n",
    "    return memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DQNAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d803118d07be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DQNAgent' is not defined"
     ]
    }
   ],
   "source": [
    "model = DQNAgent(episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((100,10),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2268616 1.3444424 1.4092128 1.3178624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiao/miniconda3/envs/pepper/lib/python2.7/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "mem = load_memory_of_episode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1916597"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem[1]['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pepper",
   "language": "python",
   "name": "pepper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
